#!/usr/bin/env python3
"""
Full-Scale 424 Targets Competition Script for Mitsui Challenge
Implements the best GPU-optimized models on the complete dataset
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import time
import json
import warnings
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import sys
import os

# Add project root to path
sys.path.append('/home/kafka/finance/mitsui-commodity-prediction-challenge')

warnings.filterwarnings('ignore')

class SharpelikeLossWithAuxiliary(nn.Module):
    """
    Combined Sharpe-like loss that achieved 0.8704 score in experiments.
    Combines Sharpe optimization with auxiliary losses for stability.
    """
    
    def __init__(self, sharpe_weight=0.7, mse_weight=0.2, mae_weight=0.1, epsilon=1e-8):
        super().__init__()
        
        self.sharpe_weight = sharpe_weight
        self.mse_weight = mse_weight
        self.mae_weight = mae_weight
        self.epsilon = epsilon
        
        self.mse_loss = nn.MSELoss()
        self.mae_loss = nn.L1Loss()
    
    def pearson_correlation(self, x, y):
        """Compute Pearson correlation coefficient (differentiable)."""
        x_centered = x - torch.mean(x)
        y_centered = y - torch.mean(y)
        
        numerator = torch.sum(x_centered * y_centered)
        denominator = torch.sqrt(torch.sum(x_centered ** 2) * torch.sum(y_centered ** 2))
        
        correlation = numerator / (denominator + self.epsilon)
        return torch.clamp(correlation, -1.0 + self.epsilon, 1.0 - self.epsilon)
    
    def forward(self, y_pred, y_true):
        """Compute combined loss."""
        batch_size, n_targets = y_pred.shape
        
        # Calculate correlations for Sharpe-like score
        correlations = []
        for i in range(n_targets):
            corr = self.pearson_correlation(y_pred[:, i], y_true[:, i])
            correlations.append(corr)
        
        correlations_tensor = torch.stack(correlations)
        mean_corr = torch.mean(correlations_tensor)
        std_corr = torch.std(correlations_tensor) + self.epsilon
        sharpe_like_score = mean_corr / std_corr
        
        # Auxiliary losses
        mse_loss = self.mse_loss(y_pred, y_true)
        mae_loss = self.mae_loss(y_pred, y_true)
        
        # Combined loss (negative Sharpe for minimization)
        total_loss = (self.sharpe_weight * (-sharpe_like_score) + 
                     self.mse_weight * mse_loss + 
                     self.mae_weight * mae_loss)
        
        return total_loss

class SharpeOptimizedModel(nn.Module):
    """Neural network optimized for Sharpe-like performance on 424 targets."""
    
    def __init__(self, input_dim, n_targets, hidden_dims=[256, 128, 64], 
                 dropout=0.2, use_batch_norm=True):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            
            layers.append(nn.ReLU())
            
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            
            prev_dim = hidden_dim
        
        # Output layer for 424 targets
        layers.append(nn.Linear(prev_dim, n_targets))
        
        self.network = nn.Sequential(*layers)
        
        # Initialize weights for better Sharpe optimization
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x):
        return self.network(x)

def manual_correlation(x, y):
    """Manual correlation calculation avoiding scipy dependencies."""
    if torch.is_tensor(x):
        x_vals = x.cpu().tolist()
    else:
        x_vals = x.tolist() if hasattr(x, 'tolist') else list(x)
        
    if torch.is_tensor(y):
        y_vals = y.cpu().tolist()
    else:
        y_vals = y.tolist() if hasattr(y, 'tolist') else list(y)
    
    n = len(x_vals)
    if n == 0:
        return 0.0
        
    sum_x = sum(x_vals)
    sum_y = sum(y_vals)
    sum_xy = sum(x_vals[i] * y_vals[i] for i in range(n))
    sum_x2 = sum(x * x for x in x_vals)
    sum_y2 = sum(y * y for y in y_vals)
    
    num = n * sum_xy - sum_x * sum_y
    den = ((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))**0.5
    
    return num / den if den != 0 else 0.0

def calculate_competition_score(y_true, y_pred):
    """Calculate the official competition Sharpe-like score."""
    correlations = []
    
    for i in range(y_true.shape[1]):
        corr = manual_correlation(y_true[:, i], y_pred[:, i])
        if abs(corr) < 1.0:  # Filter out perfect correlations
            correlations.append(corr)
    
    if len(correlations) == 0:
        return 0.0, 0.0, 0.0
    
    mean_corr = sum(correlations) / len(correlations)
    std_corr = (sum((c - mean_corr)**2 for c in correlations) / len(correlations))**0.5
    sharpe_score = mean_corr / std_corr if std_corr > 0 else mean_corr
    
    return mean_corr, std_corr, sharpe_score

def load_full_dataset():
    """Load the complete 424 targets dataset."""
    print("üìÅ Loading full competition dataset...")
    
    # Load data
    train_data = pd.read_csv('input/train.csv')
    label_data = pd.read_csv('input/train_labels.csv')
    
    print(f"   Train data shape: {train_data.shape}")
    print(f"   Label data shape: {label_data.shape}")
    
    # Merge on date_id
    merged = train_data.merge(label_data, on='date_id', how='inner')
    print(f"   Merged data shape: {merged.shape}")
    
    # Separate features and targets
    feature_cols = [col for col in train_data.columns if col != 'date_id']
    target_cols = [col for col in label_data.columns if col.startswith('target_')]
    
    print(f"   Features: {len(feature_cols)}")
    print(f"   Targets: {len(target_cols)}")
    
    # Handle missing values
    X = merged[feature_cols].fillna(method='ffill').fillna(0).values
    y = merged[target_cols].fillna(0).values
    
    print(f"‚úÖ Dataset loaded: {X.shape} features, {y.shape} targets")
    
    return X, y, feature_cols, target_cols

def create_memory_efficient_dataloaders(X, y, test_size=0.2, batch_size=64):
    """Create memory-efficient data loaders for large dataset."""
    print(f"üîÑ Creating data loaders with batch size {batch_size}...")
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, shuffle=True
    )
    
    print(f"   Train: {X_train.shape[0]} samples")
    print(f"   Test: {X_test.shape[0]} samples")
    
    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Create datasets and loaders
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_scaled), 
        torch.FloatTensor(y_train)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test_scaled), 
        torch.FloatTensor(y_test)
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=2,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=2,
        pin_memory=True
    )
    
    print("‚úÖ Data loaders created")
    
    return train_loader, test_loader, scaler, (X_test_scaled, y_test)

def train_full_scale_model(model, train_loader, test_loader, device, epochs=50, lr=0.001):
    """Train model on full 424 targets with monitoring."""
    print(f"üöÄ Training full-scale model on {device}...")
    print(f"   Epochs: {epochs}")
    print(f"   Learning rate: {lr}")
    
    # Setup training components
    criterion = SharpelikeLossWithAuxiliary()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=5, factor=0.5, verbose=True
    )
    
    # Training metrics
    train_losses = []
    test_losses = []
    sharpe_scores = []
    best_sharpe = float('-inf')
    patience_counter = 0
    
    print(f"üèÅ Starting training...")
    
    for epoch in range(epochs):
        epoch_start = time.time()
        
        # Training phase
        model.train()
        train_loss = 0
        
        for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # Gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += loss.item()
            
            # Memory management
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation phase
        model.eval()
        test_loss = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                test_loss += loss.item()
                
                all_predictions.append(outputs.cpu())
                all_targets.append(batch_y.cpu())
        
        avg_test_loss = test_loss / len(test_loader)
        test_losses.append(avg_test_loss)
        
        # Calculate Sharpe score
        predictions = torch.cat(all_predictions, dim=0).numpy()
        targets = torch.cat(all_targets, dim=0).numpy()
        
        mean_corr, std_corr, sharpe_score = calculate_competition_score(targets, predictions)
        sharpe_scores.append(sharpe_score)
        
        # Learning rate scheduling
        scheduler.step(avg_test_loss)
        
        # Early stopping and best model tracking
        if sharpe_score > best_sharpe:
            best_sharpe = sharpe_score
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            
            # Save best model
            torch.save({
                'epoch': epoch,
                'model_state_dict': best_model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'sharpe_score': best_sharpe,
                'mean_correlation': mean_corr,
                'std_correlation': std_corr,
                'train_loss': avg_train_loss,
                'test_loss': avg_test_loss
            }, 'best_424_targets_model.pth')
            
        else:
            patience_counter += 1
            if patience_counter >= 10:
                print(f"   Early stopping at epoch {epoch}")
                break
        
        epoch_time = time.time() - epoch_start
        
        # Progress reporting
        if epoch % 5 == 0 or epoch == epochs - 1:
            print(f"   Epoch {epoch:3d}: "
                  f"Train={avg_train_loss:.6f}, "
                  f"Test={avg_test_loss:.6f}, "
                  f"Sharpe={sharpe_score:.4f}, "
                  f"Time={epoch_time:.1f}s")
            
            # GPU memory status
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated(0) / (1024**3)
                memory_cached = torch.cuda.memory_reserved(0) / (1024**3)
                print(f"   GPU Memory: {memory_used:.1f}GB used, {memory_cached:.1f}GB cached")
    
    # Load best model
    if 'best_model_state' in locals():
        model.load_state_dict(best_model_state)
    
    print(f"üèÜ Training completed!")
    print(f"   Best Sharpe score: {best_sharpe:.4f}")
    print(f"   Final train loss: {train_losses[-1]:.6f}")
    print(f"   Final test loss: {test_losses[-1]:.6f}")
    
    return {
        'train_losses': train_losses,
        'test_losses': test_losses,
        'sharpe_scores': sharpe_scores,
        'best_sharpe': best_sharpe,
        'final_model': model
    }

def generate_test_predictions(model, scaler, device):
    """Generate predictions for test set."""
    print("üîÆ Generating test set predictions...")
    
    try:
        # Load test data
        test_data = pd.read_csv('input/test.csv')
        print(f"   Test data shape: {test_data.shape}")
        
        # Prepare features (same as training)
        feature_cols = [col for col in test_data.columns if col != 'date_id']
        X_test = test_data[feature_cols].fillna(method='ffill').fillna(0).values
        
        # Scale features
        X_test_scaled = scaler.transform(X_test)
        
        # Generate predictions
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
            predictions = model(X_test_tensor).cpu().numpy()
        
        # Create submission file
        submission = pd.DataFrame({
            'date_id': test_data['date_id'].values
        })
        
        # Add target predictions
        for i in range(424):
            submission[f'target_{i}'] = predictions[:, i]
        
        # Save submission
        submission.to_csv('submission_424_targets.csv', index=False)
        print(f"‚úÖ Submission saved: submission_424_targets.csv")
        print(f"   Shape: {submission.shape}")
        
        return submission
        
    except Exception as e:
        print(f"‚ùå Failed to generate test predictions: {e}")
        return None

def run_full_scale_experiment():
    """Run complete 424 targets experiment."""
    print("üéØ Full-Scale 424 Targets Experiment")
    print("=" * 70)
    
    # Check GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    
    start_time = time.time()
    
    try:
        # Load dataset
        X, y, feature_cols, target_cols = load_full_dataset()
        
        # Create data loaders
        train_loader, test_loader, scaler, (X_test, y_test) = create_memory_efficient_dataloaders(
            X, y, test_size=0.2, batch_size=32  # Smaller batch size for 424 targets
        )
        
        # Create model
        input_dim = X.shape[1]
        n_targets = y.shape[1]
        
        print(f"\nüèóÔ∏è Creating model architecture...")
        print(f"   Input features: {input_dim}")
        print(f"   Output targets: {n_targets}")
        
        model = SharpeOptimizedModel(
            input_dim=input_dim,
            n_targets=n_targets,
            hidden_dims=[512, 256, 128],  # Larger architecture for 424 targets
            dropout=0.3
        ).to(device)
        
        # Count parameters
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"   Model parameters: {total_params:,}")
        
        # Train model
        results = train_full_scale_model(
            model, train_loader, test_loader, device, epochs=100, lr=0.001
        )
        
        # Final evaluation
        print(f"\nüìä Final Evaluation on {len(target_cols)} targets:")
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test).to(device)
            final_predictions = model(X_test_tensor).cpu().numpy()
        
        final_mean_corr, final_std_corr, final_sharpe = calculate_competition_score(
            y_test, final_predictions
        )
        
        print(f"   Final Sharpe score: {final_sharpe:.4f}")
        print(f"   Mean correlation: {final_mean_corr:.4f}")
        print(f"   Std correlation: {final_std_corr:.4f}")
        
        # Generate test predictions
        submission = generate_test_predictions(model, scaler, device)
        
        # Save experiment results
        experiment_results = {
            'experiment_name': 'Full_Scale_424_Targets',
            'dataset_size': X.shape[0],
            'n_features': X.shape[1],
            'n_targets': y.shape[1],
            'model_parameters': total_params,
            'best_sharpe_score': results['best_sharpe'],
            'final_sharpe_score': final_sharpe,
            'final_mean_correlation': final_mean_corr,
            'final_std_correlation': final_std_corr,
            'training_epochs': len(results['train_losses']),
            'total_time_seconds': time.time() - start_time,
            'device': str(device)
        }
        
        with open('full_scale_424_results.json', 'w') as f:
            json.dump(experiment_results, f, indent=2)
        
        total_time = time.time() - start_time
        print(f"\nüéâ Full-Scale Experiment Completed!")
        print(f"   Total time: {total_time/60:.1f} minutes")
        print(f"   Best Sharpe score: {results['best_sharpe']:.4f}")
        print(f"   Model saved: best_424_targets_model.pth")
        print(f"   Results saved: full_scale_424_results.json")
        
        return experiment_results
        
    except Exception as e:
        print(f"‚ùå Experiment failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """Main execution."""
    print("üéØ Mitsui Challenge - Full-Scale 424 Targets Implementation")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("‚ö†Ô∏è Warning: GPU not available, using CPU")
        print("   Training 424 targets on CPU will be very slow!")
    else:
        print(f"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ CUDA Version: {torch.version.cuda}")
        print(f"‚úÖ PyTorch Version: {torch.__version__}")
    
    # Run experiment
    results = run_full_scale_experiment()
    
    if results:
        print(f"\nüèÜ Competition Model Ready!")
        print(f"   Sharpe Score: {results['final_sharpe_score']:.4f}")
        print(f"   Model: best_424_targets_model.pth")
        print(f"   Submission: submission_424_targets.csv")
    else:
        print("\n‚ùå Experiment failed!")

if __name__ == "__main__":
    main()